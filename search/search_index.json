{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is the MNE Study Template? The MNE Study Template is a full-flegded processing pipeline for your MEG and EEG data. It operates on data stored according to the Brain Imaging Data Structure (BIDS). The input is your raw data; the Study Template is configured using a simple, human-readable configuration file. When run, it will conduct preprocessing (filtering, artifact rejection), epoching, generation of evoked responses, contrasting of experimental conditions, time-frequency analysis, and source estimation. All intermediate results are saved to disk for later inspection, and an extensive report is generated. Analyses are conducted on individual (per-subject) as well as group level.","title":"Introduction"},{"location":"#what-is-the-mne-study-template","text":"The MNE Study Template is a full-flegded processing pipeline for your MEG and EEG data. It operates on data stored according to the Brain Imaging Data Structure (BIDS). The input is your raw data; the Study Template is configured using a simple, human-readable configuration file. When run, it will conduct preprocessing (filtering, artifact rejection), epoching, generation of evoked responses, contrasting of experimental conditions, time-frequency analysis, and source estimation. All intermediate results are saved to disk for later inspection, and an extensive report is generated. Analyses are conducted on individual (per-subject) as well as group level.","title":"What is the MNE Study Template?"},{"location":"settings/","text":"config Set the configuration parameters for the study. You need to define an environment variable BIDS_ROOT to point to the root of your BIDS dataset to be analyzed. acq: Optional [ str ] allow_maxshield: bool To import data that was recorded with Maxshield on before running Maxfilter set this to True . baseline: Optional [ tuple ] Specifies how to baseline-correct the epochs; if None , no baseline correction is applied. Example baseline = ( None , 0 ) # baseline between tmin and 0 bids_root: Optional [ str ] Speficy the BIDS root directory. Pass an empty string or `None to use the value specified in the BIDS_ROOT environment variable instead. Raises an exception if the BIDS root has not been specified. Example bids_root = '/path/to/your/bids_root' # Use this to specify a path here. bids_root = None # Make use of the ``BIDS_ROOT`` environment variable. ch_types: Iterable [ str ] The channel types to consider. Note Currently, MEG and EEG data cannot be processed together. Example # Use EEG channels: ch_types = [ 'eeg' ] # Use magnetometer and gradiometer MEG channels: ch_types = [ 'mag' , 'grad' ] # Currently does not work and will raise an error message: ch_types = [ 'meg' , 'eeg' ] CODE_URL conditions contrasts: Iterable [ tuple ] The conditions to contrast via a subtraction of ERPs / ERFs. Each tuple in the list corresponds to one contrast. The condition names must be specified in conditions above. Pass an empty list to avoid calculation of contrasts. Example Contrast the \"left\" and the \"right\" conditions by calculating left - right at every time point of the evoked responses: conditions = [ 'left' , 'right' ] contrasts = [( 'left' , 'right' )] # Note we pass a tuple inside the list! Contrast the \"left\" and the \"right\" conditions within the \"auditory\" and the \"visual\" modality, and \"auditory\" vs \"visual\" regardless of side: conditions = [ 'auditory/left' , 'auditory/right' , 'visual/left' , 'visual/right' ] contrasts = [( 'auditory/left' , 'auditory/right' ), ( 'visual/left' , 'visual/right' ), ( 'auditory' , 'visual' )] crop: Optional [ tuple ] If tuple, (tmin, tmax) to crop the raw data If None (default), do not crop. custom_cfg data_type: Optional [ str ] The BIDS data type. For MEG recordings, this will usually be 'meg'; and for EEG, 'eeg'. However, if your dataset contains simultaneous recordings of MEG and EEG, stored in a single file, you will typically need to set this to 'meg'. If None , we will assume that the data type matches the channel type. Example The dataset contains simultaneous recordings of MEG and EEG, and we only wish to process the EEG data, which is stored inside the MEG files: ch_types = [ 'eeg' ] data_type = 'eeg' The dataset contains simultaneous recordings of MEG and EEG, and we only wish to process the gradiometer data: ch_types = [ 'grad' ] data_type = 'meg' # or data_type = None The dataset contains only EEG data: ch_types = [ 'eeg' ] data_type = 'eeg' # or data_type = None daysback: Optional [ int ] Warning This parameter will soon be removed! Anonymization should be done on the BIDS dataset before running the Study Template! If not None , apply a time shift to dates to adjust for limitateions of FIFF files. decim: int Says how much to decimate data at the epochs level. It is typically an alternative to the resample_sfreq parameter that can be used for resampling raw data. 1 means no decimation. Good Practice / Advice Decimation requires to lowpass filtered the data to avoid aliasing. Note that using decimation is much faster than resampling. Example decim = 1 # no decimation decim = 4 # decimate by 4 ie devide sampling frequency by 4 decode: bool Whether to perform decoding (MVPA) on the contrasts specified above as \"contrasts\". MVPA will be performed on the level of individual epochs. decoding_metric: str The metric to use for cross-validation. It can be 'roc_auc' or 'accuracy' or any other metric supported by scikit-learn . With AUC, chance level is the same regardless of class balance. decoding_n_splits: int The number of folds (a.k.a. splits) to use in the cross-validation. depth: Union [ float , dict ] If float (default 0.8), it acts as the depth weighting exponent ( exp ) to use (must be between 0 and 1). None is equivalent to 0, meaning no depth weighting is performed. Can also be a dict containing additional keyword arguments to pass to :func: mne.forward.compute_depth_prior (see docstring for details and defaults). deriv_root eeg_template_montage: Optional [ str ] In situations where you wish to process EEG data and no individual digitization points (measured channel locations) are available, you can apply a \"template\" montage. This means we will assume the EEG cap was placed either according to an international system like 10/20, or as suggested by the cap manufacturers in their respective manual. Please be aware that the actual cap placement most likely deviated somewhat from the template, and, therefore, source reconstruction may be impaired. If None , do not apply a template montage. If a string, must be the name of a built-in template montage in MNE-Python. You can find an overview of supported template montages at https://mne.tools/stable/generated/mne.channels.make_standard_montage.html Example Do not apply template montage: eeg_template_montage = None Apply 64-channel Biosemi 10/20 template montage: eeg_template_montage = 'biosemi64' exclude_subjects: Iterable [ str ] Specify subjects to exclude from analysis. The MEG empty-room mock-subject is automatically excluded from regular analysis. Good Practice / Advice Keep track of the criteria leading you to exclude a participant (e.g. too many movements, missing blocks, aborted experiment, did not understand the instructions, etc, ...) The emptyroom subject will be excluded automatically. find_flat_channels_meg find_noisy_channels_meg fix_stim_artifact fsaverage_vertices h_freq: float The high-frequency cut-off in the lowpass filtering step. Keep it None if no lowpass filtering should be applied. h_trans_bandwidth: Union [ float , Literal [ 'auto' ]] Specifies the transition bandwidth of the lowpass filter. By default it's 'auto' and uses default mne parameters. ica_algorithm: Literal [ 'picard' , 'fastica' , 'extended_infomax' ] The ICA algorithm to use. ica_ctps_ecg_threshold: float The threshold parameter passed to find_bads_ecg method. ica_decim: Optional [ int ] The decimation parameter to compute ICA. If 5 it means that 1 every 5 sample is used by ICA solver. The higher the faster it is to run but the less data you have to compute a good ICA. Set to 1 or None to not perform any decimation. ica_eog_threshold: float The threshold to use during automated EOG classification. Lower values mean that more ICs will be identified as EOG-related. If too low, the false-alarm rate increases dramatically. ica_l_freq: Optional [ float ] The cutoff frequency of the high-pass filter to apply before running ICA. Using a relatively high cutoff like 1 Hz will remove slow drifts from the data, yielding improved ICA results. Set to None to not apply an additional high-pass filter. Note The filter will be applied to raw data which was already filtered according to the l_freq and h_freq settings. After filtering, the data will be epoched, and the epochs will be submitted to ICA. ica_max_iterations: int Maximum number of iterations to decompose the data into independent components. A low number means to finish earlier, but the consequence is that the algorithm may not have finished converging. To ensure convergence, pick a high number here (e.g. 3000); yet the algorithm will terminate as soon as it determines that is has successfully converged, and not necessarily exhaust the maximum number of iterations. Note that the default of 200 seems to be sufficient for Picard in many datasets, because it converges quicker than the other algorithms; but e.g. for FastICA, this limit may be too low to achieve convergence. ica_n_components: Union [ float , int ] MNE conducts ICA as a sort of a two-step procedure: First, a PCA is run on the data (trying to exclude zero-valued components in rank-deficient data); and in the second step, the principal componenets are passed to the actual ICA. You can select how many of the total principal components to pass to ICA \u2013 it can be all or just a subset. This determines how many independent components to fit, and can be controlled via this setting. If int, specifies the number of principal components that are passed to the ICA algorithm, which will be the number of independent components to fit. It must not be greater than the rank of your data (which is typically the number of channels, but may be less in some cases). If float between 0 and 1, all principal components with cumulative explained variance less than the value specified here will be passed to ICA. If None , all principal components will be used. This setting may drastically alter the time required to compute ICA. interactive: bool If True, the scripts will provide some interactive elements, such as figures. If running the scripts from a notebook or Spyder, run %matplotlib qt in the command line to open the figures in a separate window. interpolate_bads_grand_average: bool Interpolate bad sensors in each dataset before calculating the grand average. This parameter is passed to the mne.grand_average function via the keyword argument interpolate_bads . It requires to have channel locations set. Example interpolate_bads_grand_average = True inverse_method: Literal [ 'MNE' , 'dSPM' , 'sLORETA' , 'eLORETA' ] Use minimum norm, dSPM (default), sLORETA, or eLORETA. l_freq: float The low-frequency cut-off in the highpass filtering step. Keep it None if no highpass filtering should be applied. l_trans_bandwidth: Union [ float , Literal [ 'auto' ]] Specifies the transition bandwidth of the highpass filter. By default it's 'auto' and uses default mne parameters. log_date_fmt log_fmt log_level logger loose: Union [ float , Literal [ 'auto' ]] Value that weights the source variances of the dipole components that are parallel (tangential) to the cortical surface. If loose is 0 then the solution is computed with fixed orientation, and fixed must be True or \"auto\". If loose is 1, it corresponds to free orientations. The default value ('auto') is set to 0.2 for surface-oriented source space and set to 1.0 for volumetric, discrete, or mixed source spaces, unless fixed is True in which case the value 0. is used. mf_cal_fname mf_ctc_fname mf_head_origin mf_head_origin : array-like, shape (3,) | 'auto' Origin of internal and external multipolar moment space in meters. If 'auto', it will be estimated from headshape points. If automatic fitting fails (e.g., due to having too few digitization points), consider separately calling the fitting function with different options or specifying the origin manually. Example mf_head_origin = 'auto' mf_reference_run: Optional [ str ] Despite all possible care to avoid movements in the MEG, the participant will likely slowly drift down from the Dewar or slightly shift the head around in the course of the recording session. Hence, to take this into account, we are realigning all data to a single position. For this, you need to define a reference run (typically the one in the middle of the recording session). Which run to take as the reference for adjusting the head position of all runs. If None , pick the first run. Example mf_reference_run = '01' # Use run \"01\" mf_st_duration: Optional [ float ] There are two kinds of maxfiltering: SSS and tSSS [SSS = signal space separation ; tSSS = temporal signal space separation] (Taulu et al, 2004): http://cds.cern.ch/record/709081/files/0401166.pdf If not None, apply spatiotemporal SSS (tSSS) with specified buffer duration (in seconds). MaxFilter\u2122's default is 10.0 seconds in v2.2. Spatiotemporal SSS acts as implicitly as a high-pass filter where the cut-off frequency is 1/st_dur Hz. For this (and other) reasons, longer buffers are generally better as long as your system can handle the higher memory usage. To ensure that each window is processed identically, choose a buffer length that divides evenly into your data. Any data at the trailing edge that doesn't fit evenly into a whole buffer window will be lumped into the previous buffer. Good Practice / Advice If you are interested in low frequency activity (<0.1Hz), avoid using tSSS and set mf_st_duration to None . If you are interested in low frequency above 0.1 Hz, you can use the default mf_st_duration to 10 s, meaning it acts like a 0.1 Hz high-pass filter. Example mf_st_duration = None mf_st_duration = 10. # to apply tSSS with 0.1Hz highpass filter. mindist: float Exclude points closer than this distance (mm) to the bounding surface. mne_log_level msg n_boot: int The number of bootstrap resamples when estimating the standard error and confidence interval of the mean decoding score. N_JOBS: int Specifies how many subjects you want to process in parallel. new noise_cov on_error: Literal [ 'continue' , 'abort' ] Whether to abort processing as soon as an error occurs, or whether to continue with all other processing steps for as long as possible. PIPELINE_NAME proc: Optional [ str ] process_er: bool Whether to apply the same pre-processing steps to the empty-room data as to the experimental data (up until including frequency filtering). This is required if you wish to use the empty-room recording to estimate noise covariance (via noise_cov='emptyroom' ). The empty-room recording corresponding to the processed experimental data will be retrieved automatically. random_state: Optional [ int ] To specify the seed or state of the random number generator (RNG). This setting is passed to the ICA algorithm and to the decoding function, ensuring reproducible results. Set to None to avoid setting the RNG to a defined state. rec: Optional [ str ] reject rename_events resample_sfreq runs: Union [ Iterable , Literal [ 'all' ]] The runs to process. sessions: Union [ Iterable , Literal [ 'all' ]] The sessions to process. shortest_event: int Minimum number of samples an event must last. If the duration is less than this an exception will be raised. smooth: Optional [ int ] Number of iterations for the smoothing of the surface data. If None, smooth is automatically defined to fill the surface with non-zero values. The default is spacing=None. space: Optional [ str ] spacing: str The spacing to use. Can be 'ico#' for a recursively subdivided icosahedron, 'oct#' for a recursively subdivided octahedron, 'all' for all points, or an integer to use appoximate distance-based spacing (in mm). stim_artifact_tmax stim_artifact_tmin study_name: str Specify the name of your study. It will be used to populate filenames for saving the analysis results. Example study_name = 'my-study' subjects: Union [ Iterable [ str ], Literal [ 'all' ]] Subjects to analyze. If 'all' , include all subjects. To only include a subset of subjects, pass a list of their identifiers. Even if you plan on analyzing only a single subject, pass their identifier as a list. Please note that if you intend to EXCLUDE only a few subjects, you should consider setting subjects = 'all' and adding the identifiers of the excluded subjects to exclude_subjects (see next section). Example subjects = 'all' # Include all subjects. subjects = [ '05' ] # Only include subject 05. subjects = [ '01' , '02' ] # Only include subjects 01 and 02. subjects_dir: Optional [ str ] Path to the directory that contains the MRI data files and their derivativesfor all subjects. Specifically, the subjects_dir is the $SUBJECTS_DIR used by the Freesurfer software. If None , will use 'bids_root/derivatives/freesurfer/subjects' . task: str The task to process. time_frequency_conditions: Iterable [ str ] The conditions to compute time-frequency decomposition on. Example time_frequency_conditions = [ 'left' , 'right' ] tmax: float tmin use_ica: bool Whether ICA should be used or not. use_maxwell_filter use_ssp: bool Whether SSP should be used or not. VERSION failsafe_run ( on_error ) Source code in project/config.py def failsafe_run ( on_error ): def failsafe_run_decorator ( func ): @functools . wraps ( func ) # Preserve \"identity\" of original function def wrapper ( * args , ** kwargs ): try : return func ( * args , ** kwargs ) except Exception as e : message = 'A critical error occurred.' message = gen_log_message ( message = message ) if on_error == 'abort' : logger . critical ( message ) raise ( e ) elif on_error == 'debug' : logger . critical ( message ) extype , value , tb = sys . exc_info () traceback . print_exc () pdb . post_mortem ( tb ) else : message = f ' { message } The error message was: \\n { str ( e ) } ' logger . critical ( message ) return wrapper return failsafe_run_decorator gen_log_message ( message , step = None , subject = None , session = None , run = None ) Source code in project/config.py def gen_log_message ( message , step = None , subject = None , session = None , run = None ): if subject is not None : subject = f 'sub- { subject } ' if session is not None : session = f 'ses- { session } ' if run is not None : run = f 'run- { run } ' prefix = ', ' . join ([ item for item in [ subject , session , run ] if item is not None ]) if prefix : prefix = f '[ { prefix } ]' if step is not None : prefix = f '[Step- { step : 02 } ] { prefix } ' return prefix + ' ' + message get_channels_to_analyze ( info ) Return names of the channels of the channel types we wish to analyze. We also include channels marked as \"bad\" here. Source code in project/config.py def get_channels_to_analyze ( info ): \"\"\"Return names of the channels of the channel types we wish to analyze. We also include channels marked as \"bad\" here. \"\"\" # `exclude=[]`: keep \"bad\" channels, too. if get_datatype () == 'meg' and ( 'mag' in ch_types or 'grad' in ch_types or 'meg' in ch_types ): pick_idx = mne . pick_types ( info , eog = True , ecg = True , exclude = []) if 'mag' in ch_types : pick_idx += mne . pick_types ( info , meg = 'mag' , exclude = []) if 'grad' in ch_types : pick_idx += mne . pick_types ( info , meg = 'grad' , exclude = []) if 'meg' in ch_types : pick_idx = mne . pick_types ( info , meg = True , eog = True , ecg = True , exclude = []) elif ch_types == [ 'eeg' ]: pick_idx = mne . pick_types ( info , meg = False , eeg = True , eog = True , ecg = True , exclude = []) else : raise RuntimeError ( 'Something unexpected happened. Please contact ' 'the mne-study-template developers. Thank you.' ) ch_names = [ info [ 'ch_names' ][ i ] for i in pick_idx ] return ch_names get_datatype () Source code in project/config.py def get_datatype (): # Content of ch_types should be sanitized already, so we don't need any # extra sanity checks here. if data_type is not None : return data_type elif data_type is None and ch_types == [ 'eeg' ]: return 'eeg' elif data_type is None and any ([ t in [ 'meg' , 'mag' , 'grad' ] for t in ch_types ]): return 'meg' else : raise RuntimeError ( \"This probably shouldn't happen. Please contact \" \"the mne-study-template developers. Thank you.\" ) get_fs_subjects_dir () Source code in project/config.py def get_fs_subjects_dir (): if not subjects_dir : return os . path . join ( bids_root , 'derivatives' , 'freesurfer' , 'subjects' ) else : return subjects_dir get_mf_reference_run () Retrieve to run identifier (number, name) of the reference run. Source code in project/config.py def get_mf_reference_run (): \"\"\"Retrieve to run identifier (number, name) of the reference run.\"\"\" if mf_reference_run is None : # Use the first run return get_runs ()[ 0 ] else : return mf_reference_run get_reject () Source code in project/config.py def get_reject (): if reject is None : return dict () reject_ = reject . copy () # Avoid clash with global variable. if ch_types == [ 'eeg' ]: ch_types_to_remove = ( 'mag' , 'grad' ) else : ch_types_to_remove = ( 'eeg' ,) for ch_type in ch_types_to_remove : try : del reject_ [ ch_type ] except KeyError : pass return reject_ get_runs () Source code in project/config.py def get_runs (): runs_ = copy . deepcopy ( runs ) # Avoid clash with global variable. if runs_ == 'all' : runs_ = get_entity_vals ( bids_root , entity_key = 'run' ) if not runs_ : return [ None ] else : return runs_ get_sessions () Source code in project/config.py def get_sessions (): sessions_ = copy . deepcopy ( sessions ) # Avoid clash with global variable. if sessions_ == 'all' : sessions_ = get_entity_vals ( bids_root , entity_key = 'session' ) if not sessions_ : return [ None ] else : return sessions_ get_subjects () Source code in project/config.py def get_subjects (): global subjects if subjects == 'all' : s = get_entity_vals ( bids_root , entity_key = 'subject' ) else : s = subjects subjects = set ( s ) - set ( exclude_subjects ) # Drop empty-room subject. subjects = subjects - set ([ 'emptyroom' ]) return list ( subjects ) get_task () Source code in project/config.py def get_task (): if not task : tasks = get_entity_vals ( bids_root , entity_key = 'task' ) if not tasks : return None else : return tasks [ 0 ] else : return task plot_auto_scores ( auto_scores ) Plot scores of automated bad channel detection. Source code in project/config.py def plot_auto_scores ( auto_scores ): \"\"\"Plot scores of automated bad channel detection. \"\"\" import matplotlib.pyplot as plt import seaborn as sns import pandas as pd if ch_types == [ 'meg' ]: ch_types_ = [ 'grad' , 'mag' ] else : ch_types_ = ch_types figs = [] for ch_type in ch_types_ : # Only select the data for mag or grad channels. ch_subset = auto_scores [ 'ch_types' ] == ch_type ch_names = auto_scores [ 'ch_names' ][ ch_subset ] scores = auto_scores [ 'scores_noisy' ][ ch_subset ] limits = auto_scores [ 'limits_noisy' ][ ch_subset ] bins = auto_scores [ 'bins' ] # The the windows that were evaluated. # We will label each segment by its start and stop time, with up to 3 # digits before and 3 digits after the decimal place (1 ms precision). bin_labels = [ f ' { start : 3.3f } \u2013 { stop : 3.3f } ' for start , stop in bins ] # We store the data in a Pandas DataFrame. The seaborn heatmap function # we will call below will then be able to automatically assign the # correct labels to all axes. data_to_plot = pd . DataFrame ( data = scores , columns = pd . Index ( bin_labels , name = 'Time (s)' ), index = pd . Index ( ch_names , name = 'Channel' )) # First, plot the \"raw\" scores. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 8 )) fig . suptitle ( f 'Automated noisy channel detection: { ch_type } ' , fontsize = 16 , fontweight = 'bold' ) sns . heatmap ( data = data_to_plot , cmap = 'Reds' , cbar_kws = dict ( label = 'Score' ), ax = ax [ 0 ]) [ ax [ 0 ] . axvline ( x , ls = 'dashed' , lw = 0.25 , dashes = ( 25 , 15 ), color = 'gray' ) for x in range ( 1 , len ( bins ))] ax [ 0 ] . set_title ( 'All Scores' , fontweight = 'bold' ) # Now, adjust the color range to highlight segments that exceeded the # limit. sns . heatmap ( data = data_to_plot , vmin = np . nanmin ( limits ), # input data may contain NaNs cmap = 'Reds' , cbar_kws = dict ( label = 'Score' ), ax = ax [ 1 ]) [ ax [ 1 ] . axvline ( x , ls = 'dashed' , lw = 0.25 , dashes = ( 25 , 15 ), color = 'gray' ) for x in range ( 1 , len ( bins ))] ax [ 1 ] . set_title ( 'Scores > Limit' , fontweight = 'bold' ) # The figure title should not overlap with the subplots. fig . tight_layout ( rect = [ 0 , 0.03 , 1 , 0.95 ]) figs . append ( fig ) return figs","title":"Configuration Options"},{"location":"settings/#config","text":"Set the configuration parameters for the study. You need to define an environment variable BIDS_ROOT to point to the root of your BIDS dataset to be analyzed.","title":"config"},{"location":"settings/#config.acq","text":"","title":"acq"},{"location":"settings/#config.allow_maxshield","text":"To import data that was recorded with Maxshield on before running Maxfilter set this to True .","title":"allow_maxshield"},{"location":"settings/#config.baseline","text":"Specifies how to baseline-correct the epochs; if None , no baseline correction is applied. Example baseline = ( None , 0 ) # baseline between tmin and 0","title":"baseline"},{"location":"settings/#config.bids_root","text":"Speficy the BIDS root directory. Pass an empty string or `None to use the value specified in the BIDS_ROOT environment variable instead. Raises an exception if the BIDS root has not been specified. Example bids_root = '/path/to/your/bids_root' # Use this to specify a path here. bids_root = None # Make use of the ``BIDS_ROOT`` environment variable.","title":"bids_root"},{"location":"settings/#config.ch_types","text":"The channel types to consider. Note Currently, MEG and EEG data cannot be processed together. Example # Use EEG channels: ch_types = [ 'eeg' ] # Use magnetometer and gradiometer MEG channels: ch_types = [ 'mag' , 'grad' ] # Currently does not work and will raise an error message: ch_types = [ 'meg' , 'eeg' ]","title":"ch_types"},{"location":"settings/#config.CODE_URL","text":"","title":"CODE_URL"},{"location":"settings/#config.conditions","text":"","title":"conditions"},{"location":"settings/#config.contrasts","text":"The conditions to contrast via a subtraction of ERPs / ERFs. Each tuple in the list corresponds to one contrast. The condition names must be specified in conditions above. Pass an empty list to avoid calculation of contrasts. Example Contrast the \"left\" and the \"right\" conditions by calculating left - right at every time point of the evoked responses: conditions = [ 'left' , 'right' ] contrasts = [( 'left' , 'right' )] # Note we pass a tuple inside the list! Contrast the \"left\" and the \"right\" conditions within the \"auditory\" and the \"visual\" modality, and \"auditory\" vs \"visual\" regardless of side: conditions = [ 'auditory/left' , 'auditory/right' , 'visual/left' , 'visual/right' ] contrasts = [( 'auditory/left' , 'auditory/right' ), ( 'visual/left' , 'visual/right' ), ( 'auditory' , 'visual' )]","title":"contrasts"},{"location":"settings/#config.crop","text":"If tuple, (tmin, tmax) to crop the raw data If None (default), do not crop.","title":"crop"},{"location":"settings/#config.custom_cfg","text":"","title":"custom_cfg"},{"location":"settings/#config.data_type","text":"The BIDS data type. For MEG recordings, this will usually be 'meg'; and for EEG, 'eeg'. However, if your dataset contains simultaneous recordings of MEG and EEG, stored in a single file, you will typically need to set this to 'meg'. If None , we will assume that the data type matches the channel type. Example The dataset contains simultaneous recordings of MEG and EEG, and we only wish to process the EEG data, which is stored inside the MEG files: ch_types = [ 'eeg' ] data_type = 'eeg' The dataset contains simultaneous recordings of MEG and EEG, and we only wish to process the gradiometer data: ch_types = [ 'grad' ] data_type = 'meg' # or data_type = None The dataset contains only EEG data: ch_types = [ 'eeg' ] data_type = 'eeg' # or data_type = None","title":"data_type"},{"location":"settings/#config.daysback","text":"Warning This parameter will soon be removed! Anonymization should be done on the BIDS dataset before running the Study Template! If not None , apply a time shift to dates to adjust for limitateions of FIFF files.","title":"daysback"},{"location":"settings/#config.decim","text":"Says how much to decimate data at the epochs level. It is typically an alternative to the resample_sfreq parameter that can be used for resampling raw data. 1 means no decimation. Good Practice / Advice Decimation requires to lowpass filtered the data to avoid aliasing. Note that using decimation is much faster than resampling. Example decim = 1 # no decimation decim = 4 # decimate by 4 ie devide sampling frequency by 4","title":"decim"},{"location":"settings/#config.decode","text":"Whether to perform decoding (MVPA) on the contrasts specified above as \"contrasts\". MVPA will be performed on the level of individual epochs.","title":"decode"},{"location":"settings/#config.decoding_metric","text":"The metric to use for cross-validation. It can be 'roc_auc' or 'accuracy' or any other metric supported by scikit-learn . With AUC, chance level is the same regardless of class balance.","title":"decoding_metric"},{"location":"settings/#config.decoding_n_splits","text":"The number of folds (a.k.a. splits) to use in the cross-validation.","title":"decoding_n_splits"},{"location":"settings/#config.depth","text":"If float (default 0.8), it acts as the depth weighting exponent ( exp ) to use (must be between 0 and 1). None is equivalent to 0, meaning no depth weighting is performed. Can also be a dict containing additional keyword arguments to pass to :func: mne.forward.compute_depth_prior (see docstring for details and defaults).","title":"depth"},{"location":"settings/#config.deriv_root","text":"","title":"deriv_root"},{"location":"settings/#config.eeg_template_montage","text":"In situations where you wish to process EEG data and no individual digitization points (measured channel locations) are available, you can apply a \"template\" montage. This means we will assume the EEG cap was placed either according to an international system like 10/20, or as suggested by the cap manufacturers in their respective manual. Please be aware that the actual cap placement most likely deviated somewhat from the template, and, therefore, source reconstruction may be impaired. If None , do not apply a template montage. If a string, must be the name of a built-in template montage in MNE-Python. You can find an overview of supported template montages at https://mne.tools/stable/generated/mne.channels.make_standard_montage.html Example Do not apply template montage: eeg_template_montage = None Apply 64-channel Biosemi 10/20 template montage: eeg_template_montage = 'biosemi64'","title":"eeg_template_montage"},{"location":"settings/#config.exclude_subjects","text":"Specify subjects to exclude from analysis. The MEG empty-room mock-subject is automatically excluded from regular analysis. Good Practice / Advice Keep track of the criteria leading you to exclude a participant (e.g. too many movements, missing blocks, aborted experiment, did not understand the instructions, etc, ...) The emptyroom subject will be excluded automatically.","title":"exclude_subjects"},{"location":"settings/#config.find_flat_channels_meg","text":"","title":"find_flat_channels_meg"},{"location":"settings/#config.find_noisy_channels_meg","text":"","title":"find_noisy_channels_meg"},{"location":"settings/#config.fix_stim_artifact","text":"","title":"fix_stim_artifact"},{"location":"settings/#config.fsaverage_vertices","text":"","title":"fsaverage_vertices"},{"location":"settings/#config.h_freq","text":"The high-frequency cut-off in the lowpass filtering step. Keep it None if no lowpass filtering should be applied.","title":"h_freq"},{"location":"settings/#config.h_trans_bandwidth","text":"Specifies the transition bandwidth of the lowpass filter. By default it's 'auto' and uses default mne parameters.","title":"h_trans_bandwidth"},{"location":"settings/#config.ica_algorithm","text":"The ICA algorithm to use.","title":"ica_algorithm"},{"location":"settings/#config.ica_ctps_ecg_threshold","text":"The threshold parameter passed to find_bads_ecg method.","title":"ica_ctps_ecg_threshold"},{"location":"settings/#config.ica_decim","text":"The decimation parameter to compute ICA. If 5 it means that 1 every 5 sample is used by ICA solver. The higher the faster it is to run but the less data you have to compute a good ICA. Set to 1 or None to not perform any decimation.","title":"ica_decim"},{"location":"settings/#config.ica_eog_threshold","text":"The threshold to use during automated EOG classification. Lower values mean that more ICs will be identified as EOG-related. If too low, the false-alarm rate increases dramatically.","title":"ica_eog_threshold"},{"location":"settings/#config.ica_l_freq","text":"The cutoff frequency of the high-pass filter to apply before running ICA. Using a relatively high cutoff like 1 Hz will remove slow drifts from the data, yielding improved ICA results. Set to None to not apply an additional high-pass filter. Note The filter will be applied to raw data which was already filtered according to the l_freq and h_freq settings. After filtering, the data will be epoched, and the epochs will be submitted to ICA.","title":"ica_l_freq"},{"location":"settings/#config.ica_max_iterations","text":"Maximum number of iterations to decompose the data into independent components. A low number means to finish earlier, but the consequence is that the algorithm may not have finished converging. To ensure convergence, pick a high number here (e.g. 3000); yet the algorithm will terminate as soon as it determines that is has successfully converged, and not necessarily exhaust the maximum number of iterations. Note that the default of 200 seems to be sufficient for Picard in many datasets, because it converges quicker than the other algorithms; but e.g. for FastICA, this limit may be too low to achieve convergence.","title":"ica_max_iterations"},{"location":"settings/#config.ica_n_components","text":"MNE conducts ICA as a sort of a two-step procedure: First, a PCA is run on the data (trying to exclude zero-valued components in rank-deficient data); and in the second step, the principal componenets are passed to the actual ICA. You can select how many of the total principal components to pass to ICA \u2013 it can be all or just a subset. This determines how many independent components to fit, and can be controlled via this setting. If int, specifies the number of principal components that are passed to the ICA algorithm, which will be the number of independent components to fit. It must not be greater than the rank of your data (which is typically the number of channels, but may be less in some cases). If float between 0 and 1, all principal components with cumulative explained variance less than the value specified here will be passed to ICA. If None , all principal components will be used. This setting may drastically alter the time required to compute ICA.","title":"ica_n_components"},{"location":"settings/#config.interactive","text":"If True, the scripts will provide some interactive elements, such as figures. If running the scripts from a notebook or Spyder, run %matplotlib qt in the command line to open the figures in a separate window.","title":"interactive"},{"location":"settings/#config.interpolate_bads_grand_average","text":"Interpolate bad sensors in each dataset before calculating the grand average. This parameter is passed to the mne.grand_average function via the keyword argument interpolate_bads . It requires to have channel locations set. Example interpolate_bads_grand_average = True","title":"interpolate_bads_grand_average"},{"location":"settings/#config.inverse_method","text":"Use minimum norm, dSPM (default), sLORETA, or eLORETA.","title":"inverse_method"},{"location":"settings/#config.l_freq","text":"The low-frequency cut-off in the highpass filtering step. Keep it None if no highpass filtering should be applied.","title":"l_freq"},{"location":"settings/#config.l_trans_bandwidth","text":"Specifies the transition bandwidth of the highpass filter. By default it's 'auto' and uses default mne parameters.","title":"l_trans_bandwidth"},{"location":"settings/#config.log_date_fmt","text":"","title":"log_date_fmt"},{"location":"settings/#config.log_fmt","text":"","title":"log_fmt"},{"location":"settings/#config.log_level","text":"","title":"log_level"},{"location":"settings/#config.logger","text":"","title":"logger"},{"location":"settings/#config.loose","text":"Value that weights the source variances of the dipole components that are parallel (tangential) to the cortical surface. If loose is 0 then the solution is computed with fixed orientation, and fixed must be True or \"auto\". If loose is 1, it corresponds to free orientations. The default value ('auto') is set to 0.2 for surface-oriented source space and set to 1.0 for volumetric, discrete, or mixed source spaces, unless fixed is True in which case the value 0. is used.","title":"loose"},{"location":"settings/#config.mf_cal_fname","text":"","title":"mf_cal_fname"},{"location":"settings/#config.mf_ctc_fname","text":"","title":"mf_ctc_fname"},{"location":"settings/#config.mf_head_origin","text":"mf_head_origin : array-like, shape (3,) | 'auto' Origin of internal and external multipolar moment space in meters. If 'auto', it will be estimated from headshape points. If automatic fitting fails (e.g., due to having too few digitization points), consider separately calling the fitting function with different options or specifying the origin manually. Example mf_head_origin = 'auto'","title":"mf_head_origin"},{"location":"settings/#config.mf_reference_run","text":"Despite all possible care to avoid movements in the MEG, the participant will likely slowly drift down from the Dewar or slightly shift the head around in the course of the recording session. Hence, to take this into account, we are realigning all data to a single position. For this, you need to define a reference run (typically the one in the middle of the recording session). Which run to take as the reference for adjusting the head position of all runs. If None , pick the first run. Example mf_reference_run = '01' # Use run \"01\"","title":"mf_reference_run"},{"location":"settings/#config.mf_st_duration","text":"There are two kinds of maxfiltering: SSS and tSSS [SSS = signal space separation ; tSSS = temporal signal space separation] (Taulu et al, 2004): http://cds.cern.ch/record/709081/files/0401166.pdf If not None, apply spatiotemporal SSS (tSSS) with specified buffer duration (in seconds). MaxFilter\u2122's default is 10.0 seconds in v2.2. Spatiotemporal SSS acts as implicitly as a high-pass filter where the cut-off frequency is 1/st_dur Hz. For this (and other) reasons, longer buffers are generally better as long as your system can handle the higher memory usage. To ensure that each window is processed identically, choose a buffer length that divides evenly into your data. Any data at the trailing edge that doesn't fit evenly into a whole buffer window will be lumped into the previous buffer. Good Practice / Advice If you are interested in low frequency activity (<0.1Hz), avoid using tSSS and set mf_st_duration to None . If you are interested in low frequency above 0.1 Hz, you can use the default mf_st_duration to 10 s, meaning it acts like a 0.1 Hz high-pass filter. Example mf_st_duration = None mf_st_duration = 10. # to apply tSSS with 0.1Hz highpass filter.","title":"mf_st_duration"},{"location":"settings/#config.mindist","text":"Exclude points closer than this distance (mm) to the bounding surface.","title":"mindist"},{"location":"settings/#config.mne_log_level","text":"","title":"mne_log_level"},{"location":"settings/#config.msg","text":"","title":"msg"},{"location":"settings/#config.n_boot","text":"The number of bootstrap resamples when estimating the standard error and confidence interval of the mean decoding score.","title":"n_boot"},{"location":"settings/#config.N_JOBS","text":"Specifies how many subjects you want to process in parallel.","title":"N_JOBS"},{"location":"settings/#config.new","text":"","title":"new"},{"location":"settings/#config.noise_cov","text":"","title":"noise_cov"},{"location":"settings/#config.on_error","text":"Whether to abort processing as soon as an error occurs, or whether to continue with all other processing steps for as long as possible.","title":"on_error"},{"location":"settings/#config.PIPELINE_NAME","text":"","title":"PIPELINE_NAME"},{"location":"settings/#config.proc","text":"","title":"proc"},{"location":"settings/#config.process_er","text":"Whether to apply the same pre-processing steps to the empty-room data as to the experimental data (up until including frequency filtering). This is required if you wish to use the empty-room recording to estimate noise covariance (via noise_cov='emptyroom' ). The empty-room recording corresponding to the processed experimental data will be retrieved automatically.","title":"process_er"},{"location":"settings/#config.random_state","text":"To specify the seed or state of the random number generator (RNG). This setting is passed to the ICA algorithm and to the decoding function, ensuring reproducible results. Set to None to avoid setting the RNG to a defined state.","title":"random_state"},{"location":"settings/#config.rec","text":"","title":"rec"},{"location":"settings/#config.reject","text":"","title":"reject"},{"location":"settings/#config.rename_events","text":"","title":"rename_events"},{"location":"settings/#config.resample_sfreq","text":"","title":"resample_sfreq"},{"location":"settings/#config.runs","text":"The runs to process.","title":"runs"},{"location":"settings/#config.sessions","text":"The sessions to process.","title":"sessions"},{"location":"settings/#config.shortest_event","text":"Minimum number of samples an event must last. If the duration is less than this an exception will be raised.","title":"shortest_event"},{"location":"settings/#config.smooth","text":"Number of iterations for the smoothing of the surface data. If None, smooth is automatically defined to fill the surface with non-zero values. The default is spacing=None.","title":"smooth"},{"location":"settings/#config.space","text":"","title":"space"},{"location":"settings/#config.spacing","text":"","title":"spacing"},{"location":"settings/#the-spacing-to-use-can-be-ico-for-a-recursively-subdivided","text":"","title":"The spacing to use. Can be 'ico#' for a recursively subdivided"},{"location":"settings/#icosahedron-oct-for-a-recursively-subdivided-octahedron","text":"","title":"icosahedron, 'oct#' for a recursively subdivided octahedron,"},{"location":"settings/#all-for-all-points-or-an-integer-to-use-appoximate","text":"","title":"'all' for all points, or an integer to use appoximate"},{"location":"settings/#distance-based-spacing-in-mm","text":"","title":"distance-based spacing (in mm)."},{"location":"settings/#config.stim_artifact_tmax","text":"","title":"stim_artifact_tmax"},{"location":"settings/#config.stim_artifact_tmin","text":"","title":"stim_artifact_tmin"},{"location":"settings/#config.study_name","text":"Specify the name of your study. It will be used to populate filenames for saving the analysis results. Example study_name = 'my-study'","title":"study_name"},{"location":"settings/#config.subjects","text":"Subjects to analyze. If 'all' , include all subjects. To only include a subset of subjects, pass a list of their identifiers. Even if you plan on analyzing only a single subject, pass their identifier as a list. Please note that if you intend to EXCLUDE only a few subjects, you should consider setting subjects = 'all' and adding the identifiers of the excluded subjects to exclude_subjects (see next section). Example subjects = 'all' # Include all subjects. subjects = [ '05' ] # Only include subject 05. subjects = [ '01' , '02' ] # Only include subjects 01 and 02.","title":"subjects"},{"location":"settings/#config.subjects_dir","text":"Path to the directory that contains the MRI data files and their derivativesfor all subjects. Specifically, the subjects_dir is the $SUBJECTS_DIR used by the Freesurfer software. If None , will use 'bids_root/derivatives/freesurfer/subjects' .","title":"subjects_dir"},{"location":"settings/#config.task","text":"The task to process.","title":"task"},{"location":"settings/#config.time_frequency_conditions","text":"The conditions to compute time-frequency decomposition on. Example time_frequency_conditions = [ 'left' , 'right' ]","title":"time_frequency_conditions"},{"location":"settings/#config.tmax","text":"","title":"tmax"},{"location":"settings/#config.tmin","text":"","title":"tmin"},{"location":"settings/#config.use_ica","text":"Whether ICA should be used or not.","title":"use_ica"},{"location":"settings/#config.use_maxwell_filter","text":"","title":"use_maxwell_filter"},{"location":"settings/#config.use_ssp","text":"Whether SSP should be used or not.","title":"use_ssp"},{"location":"settings/#config.VERSION","text":"","title":"VERSION"},{"location":"settings/#config.failsafe_run","text":"Source code in project/config.py def failsafe_run ( on_error ): def failsafe_run_decorator ( func ): @functools . wraps ( func ) # Preserve \"identity\" of original function def wrapper ( * args , ** kwargs ): try : return func ( * args , ** kwargs ) except Exception as e : message = 'A critical error occurred.' message = gen_log_message ( message = message ) if on_error == 'abort' : logger . critical ( message ) raise ( e ) elif on_error == 'debug' : logger . critical ( message ) extype , value , tb = sys . exc_info () traceback . print_exc () pdb . post_mortem ( tb ) else : message = f ' { message } The error message was: \\n { str ( e ) } ' logger . critical ( message ) return wrapper return failsafe_run_decorator","title":"failsafe_run()"},{"location":"settings/#config.gen_log_message","text":"Source code in project/config.py def gen_log_message ( message , step = None , subject = None , session = None , run = None ): if subject is not None : subject = f 'sub- { subject } ' if session is not None : session = f 'ses- { session } ' if run is not None : run = f 'run- { run } ' prefix = ', ' . join ([ item for item in [ subject , session , run ] if item is not None ]) if prefix : prefix = f '[ { prefix } ]' if step is not None : prefix = f '[Step- { step : 02 } ] { prefix } ' return prefix + ' ' + message","title":"gen_log_message()"},{"location":"settings/#config.get_channels_to_analyze","text":"Return names of the channels of the channel types we wish to analyze. We also include channels marked as \"bad\" here. Source code in project/config.py def get_channels_to_analyze ( info ): \"\"\"Return names of the channels of the channel types we wish to analyze. We also include channels marked as \"bad\" here. \"\"\" # `exclude=[]`: keep \"bad\" channels, too. if get_datatype () == 'meg' and ( 'mag' in ch_types or 'grad' in ch_types or 'meg' in ch_types ): pick_idx = mne . pick_types ( info , eog = True , ecg = True , exclude = []) if 'mag' in ch_types : pick_idx += mne . pick_types ( info , meg = 'mag' , exclude = []) if 'grad' in ch_types : pick_idx += mne . pick_types ( info , meg = 'grad' , exclude = []) if 'meg' in ch_types : pick_idx = mne . pick_types ( info , meg = True , eog = True , ecg = True , exclude = []) elif ch_types == [ 'eeg' ]: pick_idx = mne . pick_types ( info , meg = False , eeg = True , eog = True , ecg = True , exclude = []) else : raise RuntimeError ( 'Something unexpected happened. Please contact ' 'the mne-study-template developers. Thank you.' ) ch_names = [ info [ 'ch_names' ][ i ] for i in pick_idx ] return ch_names","title":"get_channels_to_analyze()"},{"location":"settings/#config.get_datatype","text":"Source code in project/config.py def get_datatype (): # Content of ch_types should be sanitized already, so we don't need any # extra sanity checks here. if data_type is not None : return data_type elif data_type is None and ch_types == [ 'eeg' ]: return 'eeg' elif data_type is None and any ([ t in [ 'meg' , 'mag' , 'grad' ] for t in ch_types ]): return 'meg' else : raise RuntimeError ( \"This probably shouldn't happen. Please contact \" \"the mne-study-template developers. Thank you.\" )","title":"get_datatype()"},{"location":"settings/#config.get_fs_subjects_dir","text":"Source code in project/config.py def get_fs_subjects_dir (): if not subjects_dir : return os . path . join ( bids_root , 'derivatives' , 'freesurfer' , 'subjects' ) else : return subjects_dir","title":"get_fs_subjects_dir()"},{"location":"settings/#config.get_mf_reference_run","text":"Retrieve to run identifier (number, name) of the reference run. Source code in project/config.py def get_mf_reference_run (): \"\"\"Retrieve to run identifier (number, name) of the reference run.\"\"\" if mf_reference_run is None : # Use the first run return get_runs ()[ 0 ] else : return mf_reference_run","title":"get_mf_reference_run()"},{"location":"settings/#config.get_reject","text":"Source code in project/config.py def get_reject (): if reject is None : return dict () reject_ = reject . copy () # Avoid clash with global variable. if ch_types == [ 'eeg' ]: ch_types_to_remove = ( 'mag' , 'grad' ) else : ch_types_to_remove = ( 'eeg' ,) for ch_type in ch_types_to_remove : try : del reject_ [ ch_type ] except KeyError : pass return reject_","title":"get_reject()"},{"location":"settings/#config.get_runs","text":"Source code in project/config.py def get_runs (): runs_ = copy . deepcopy ( runs ) # Avoid clash with global variable. if runs_ == 'all' : runs_ = get_entity_vals ( bids_root , entity_key = 'run' ) if not runs_ : return [ None ] else : return runs_","title":"get_runs()"},{"location":"settings/#config.get_sessions","text":"Source code in project/config.py def get_sessions (): sessions_ = copy . deepcopy ( sessions ) # Avoid clash with global variable. if sessions_ == 'all' : sessions_ = get_entity_vals ( bids_root , entity_key = 'session' ) if not sessions_ : return [ None ] else : return sessions_","title":"get_sessions()"},{"location":"settings/#config.get_subjects","text":"Source code in project/config.py def get_subjects (): global subjects if subjects == 'all' : s = get_entity_vals ( bids_root , entity_key = 'subject' ) else : s = subjects subjects = set ( s ) - set ( exclude_subjects ) # Drop empty-room subject. subjects = subjects - set ([ 'emptyroom' ]) return list ( subjects )","title":"get_subjects()"},{"location":"settings/#config.get_task","text":"Source code in project/config.py def get_task (): if not task : tasks = get_entity_vals ( bids_root , entity_key = 'task' ) if not tasks : return None else : return tasks [ 0 ] else : return task","title":"get_task()"},{"location":"settings/#config.plot_auto_scores","text":"Plot scores of automated bad channel detection. Source code in project/config.py def plot_auto_scores ( auto_scores ): \"\"\"Plot scores of automated bad channel detection. \"\"\" import matplotlib.pyplot as plt import seaborn as sns import pandas as pd if ch_types == [ 'meg' ]: ch_types_ = [ 'grad' , 'mag' ] else : ch_types_ = ch_types figs = [] for ch_type in ch_types_ : # Only select the data for mag or grad channels. ch_subset = auto_scores [ 'ch_types' ] == ch_type ch_names = auto_scores [ 'ch_names' ][ ch_subset ] scores = auto_scores [ 'scores_noisy' ][ ch_subset ] limits = auto_scores [ 'limits_noisy' ][ ch_subset ] bins = auto_scores [ 'bins' ] # The the windows that were evaluated. # We will label each segment by its start and stop time, with up to 3 # digits before and 3 digits after the decimal place (1 ms precision). bin_labels = [ f ' { start : 3.3f } \u2013 { stop : 3.3f } ' for start , stop in bins ] # We store the data in a Pandas DataFrame. The seaborn heatmap function # we will call below will then be able to automatically assign the # correct labels to all axes. data_to_plot = pd . DataFrame ( data = scores , columns = pd . Index ( bin_labels , name = 'Time (s)' ), index = pd . Index ( ch_names , name = 'Channel' )) # First, plot the \"raw\" scores. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 8 )) fig . suptitle ( f 'Automated noisy channel detection: { ch_type } ' , fontsize = 16 , fontweight = 'bold' ) sns . heatmap ( data = data_to_plot , cmap = 'Reds' , cbar_kws = dict ( label = 'Score' ), ax = ax [ 0 ]) [ ax [ 0 ] . axvline ( x , ls = 'dashed' , lw = 0.25 , dashes = ( 25 , 15 ), color = 'gray' ) for x in range ( 1 , len ( bins ))] ax [ 0 ] . set_title ( 'All Scores' , fontweight = 'bold' ) # Now, adjust the color range to highlight segments that exceeded the # limit. sns . heatmap ( data = data_to_plot , vmin = np . nanmin ( limits ), # input data may contain NaNs cmap = 'Reds' , cbar_kws = dict ( label = 'Score' ), ax = ax [ 1 ]) [ ax [ 1 ] . axvline ( x , ls = 'dashed' , lw = 0.25 , dashes = ( 25 , 15 ), color = 'gray' ) for x in range ( 1 , len ( bins ))] ax [ 1 ] . set_title ( 'Scores > Limit' , fontweight = 'bold' ) # The figure title should not overlap with the subplots. fig . tight_layout ( rect = [ 0 , 0.03 , 1 , 0.95 ]) figs . append ( fig ) return figs","title":"plot_auto_scores()"}]}